---
title : Project: automatic error propagation
author : |
  Michiel Stock
  Bram De Jaegher
  Daan?
date: December 2019
---

# What is error propagation?

During my first year of college, I really liked the physics lectures because everything was exact. In my first year of physics, I really disliked the physics labs, because everything was messy. First-year physics labs are generally disliked partially because of the uninspiring topics (measuring resistance in a wire! determining a heat transfer coefficient! measuring chirality of sugar!) and partly because this was the only lab in which 'exact' measurements had to be performed. These labs introduced the complex and unexplained rules for measurement error propagation. Without any prior statistics or probability courses, it was lost on me where these strange rules originated from, as sharp contrast to nearly everything else in physics.

A year later, after plowing through a basic probability course, error propagation is less mysterious. Almost all these rules to account for the error can be derived from a simple principle:

> Given two **independent** random variables $$X$$ and $$Y$$, the variance of a linear combination $$\text{Var}[aX+bY]=a^2\text{Var}[X] + b^2\text{Var}[Y]$$.

Measurement errors are usually given by a standard deviation, the square root of the variance. Given this principle, error propagation is merely bookkeeping of the standard error on the measurement for various computations.

For nonlinear functions, we can compute an approximate uncertainty propagation using a first-order Taylor approximation. We have, for any function $$f(x)$$:

$$
f(x\pm \sigma) \approx f(x) \pm |f'(x)|\sigma\,.
$$

For example, for squaring a function, we have

$$
(x\pm\sigma)^2 = x^2 \pm 2|x|\sigma\,.
$$

Note that this is consistent with the above rules for multiplication. Let us implement the general formula for raising a measurement to the power $$p$$.




We can implement this for all the standard mathematical functions one by one. However, Julia provides use with two efficient tools to do this in one swoop: automatic differentiation and metaprogramming. We just loop a list of functions of interest and automatically generate the correct approximate rule.


```julia
using ForwardDiff

for f in [:sin, :cos, :tan, :exp, :log, :log2, :log10, :sqrt, :inv]
    eval(quote
        # this is a line of code generated using string interpolation
        Base.$f(m::Measurement) = $f(m.x) ± abs(ForwardDiff.derivative($f, m.x) * m.σ)
    end)
end
```

Instead of processing the measurements and the standard error separately, suppose we could just make a new type of number which contains both the observed value and its uncertainty. And suppose we could just compute something with these numbers, plugging them into our formulas where the error is automatically accounted for using the standard error propagation rules. In Julia, it is dead simple to construct such new numbers and just overload existing functions such that they compute in the correct way.

# Goal of this project

We will implement a binary operator `±`, which can be used to add the standard error to a measurement.
The result is a new type `Measurement` containing both the value and the standard error. Standard functions will be overloaded to process this structure correctly.

# Assignments

1. Make an `Measurement` structure with two fields: the measured value `x` and its standard deviation `σ`. Make sure that both `x` and `σ` are of the same type and a subtype of `Real`.
2. Make a constructor for `Measurement`, such that an error is returned when a negative standard error is given.
3. Make two functions `val` and `err`, which respecitivly return the value and the standard error of a measurement.
4. Make a binary operator `±` (`\pm<TAB>`), such that `x ± σ` returns a new instance of the type `Measurement`. Try it on a value 4.0 with a standard error of 1.2.
5. Overload all functions of scalar multiplication, adding and substracting measurements and adding a constant such that they correctly process measurements. Try some examples.
6. ^: nonlinear function
7. Run example meta programming.
8. Exercis. with data.

```julia
# scalar multiplication
Base.:*(a::Real, m::Measurement) = a * m.x ± abs(a) * m.σ;
Base.:/(m::Measurement, a::Real) = inv(a) * m;
# adding and substracting measurements
Base.:+(m1::Measurement, m2::Measurement) = (m1.x + m2.x) ± √(m1.σ^2 + m2.σ^2);
Base.:-(m1::Measurement, m2::Measurement) = (m1.x - m2.x) ± √(m1.σ^2 + m2.σ^2);
Base.:-(m::Measurement) = -m.x ± m.σ;
# adding a constant
Base.:+(m::Measurement, a::Real) = m + (a ± zero(a));
Base.:+(a::Real, m::Measurement) = m + a;
# multiplying two measurments
Base.:*(m1::Measurement, m2::Measurement) = m1.x * m2.x ± (m1.x * m2.x) * √((m1.σ / m1.x)^2 + (m2.σ / m2.x)^2);
```

```julia
Base.:^(m::Measurement, p::Integer) = m.x^2 ± abs(p*m.x^(p-1) * m.σ);
```


```julia
m^2
```
